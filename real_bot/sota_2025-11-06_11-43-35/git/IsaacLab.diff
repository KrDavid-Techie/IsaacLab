--- git status ---
On branch main
Your branch is behind 'origin/main' by 6 commits, and can be fast-forwarded.
  (use "git pull" to update your local branch)

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   .gitignore
	modified:   scripts/reinforcement_learning/rsl_rl/play.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/flat_env_cfg.py
	modified:   source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/rough_env_cfg.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	2.7.0+cu128
	environment.yml.bak
	scripts/pyramid_train.py
	scripts/sensor_prepare.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/.gitignore b/.gitignore
index 08d2e8dee5..604a657363 100644
--- a/.gitignore
+++ b/.gitignore
@@ -69,3 +69,4 @@ tests/
 
 # Docker history
 .isaac-lab-docker-history
+.github/copilot-instructions.md
diff --git a/scripts/reinforcement_learning/rsl_rl/play.py b/scripts/reinforcement_learning/rsl_rl/play.py
index 11ef739946..91e2f0573b 100644
--- a/scripts/reinforcement_learning/rsl_rl/play.py
+++ b/scripts/reinforcement_learning/rsl_rl/play.py
@@ -34,6 +34,9 @@ parser.add_argument(
     help="Use the pre-trained checkpoint from Nucleus.",
 )
 parser.add_argument("--real-time", action="store_true", default=False, help="Run in real-time, if possible.")
+parser.add_argument("--log_joints", action="store_true", default=False, help="Log joint positions and velocities.")
+parser.add_argument("--log_commands", action="store_true", default=False, help="Log velocity commands.")
+parser.add_argument("--log_interval", type=int, default=10, help="Interval (in steps) for logging joint data.")
 # append RSL-RL cli arguments
 cli_args.add_rsl_rl_args(parser)
 # append AppLauncher cli args
@@ -57,6 +60,8 @@ import gymnasium as gym
 import os
 import time
 import torch
+import csv
+import numpy as np
 
 from rsl_rl.runners import DistillationRunner, OnPolicyRunner
 
@@ -174,6 +179,59 @@ def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agen
 
     dt = env.unwrapped.step_dt
 
+    # setup joint logging if requested
+    joint_log_files = []
+    joint_writers = []
+    if args_cli.log_joints:
+        joint_log_dir = os.path.join(log_dir, "joint_logs")
+        os.makedirs(joint_log_dir, exist_ok=True)
+        
+        # Get joint names from the environment
+        robot = env.unwrapped.scene["robot"]
+        joint_names = robot.data.joint_names
+        num_envs = env.unwrapped.scene.num_envs
+        
+        print(f"[INFO] Logging joint data for {num_envs} environments")
+        print(f"[INFO] Joint names: {joint_names}")
+        print(f"[INFO] Logging interval: {args_cli.log_interval} steps")
+        
+        # Create CSV file for each environment
+        for env_idx in range(num_envs):
+            log_file_path = os.path.join(joint_log_dir, f"env_{env_idx:03d}_joints.csv")
+            log_file = open(log_file_path, 'w', newline='')
+            joint_log_files.append(log_file)
+            
+            # Create CSV writer with header
+            fieldnames = ['timestep', 'time'] + [f"{name}_pos" for name in joint_names] + [f"{name}_vel" for name in joint_names]
+            writer = csv.DictWriter(log_file, fieldnames=fieldnames)
+            writer.writeheader()
+            joint_writers.append(writer)
+    
+    # setup command logging if requested
+    command_log_files = []
+    command_writers = []
+    if args_cli.log_commands:
+        command_log_dir = os.path.join(log_dir, "command_logs")
+        os.makedirs(command_log_dir, exist_ok=True)
+        
+        num_envs = env.unwrapped.scene.num_envs
+        
+        print(f"[INFO] Logging command data for {num_envs} environments")
+        print(f"[INFO] Command logging interval: {args_cli.log_interval} steps")
+        
+        # Create CSV file for each environment
+        for env_idx in range(num_envs):
+            log_file_path = os.path.join(command_log_dir, f"env_{env_idx:03d}_commands.csv")
+            log_file = open(log_file_path, 'w', newline='')
+            command_log_files.append(log_file)
+            
+            # Create CSV writer with header
+            fieldnames = ['timestep', 'time', 'cmd_lin_vel_x', 'cmd_lin_vel_y', 'cmd_ang_vel_z', 
+                         'actual_lin_vel_x', 'actual_lin_vel_y', 'actual_ang_vel_z']
+            writer = csv.DictWriter(log_file, fieldnames=fieldnames)
+            writer.writeheader()
+            command_writers.append(writer)
+
     # reset environment
     obs = env.get_observations()
     timestep = 0
@@ -186,8 +244,73 @@ def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agen
             actions = policy(obs)
             # env stepping
             obs, _, _, _ = env.step(actions)
+        
+        # log joint data if requested
+        if args_cli.log_joints and timestep % args_cli.log_interval == 0:
+            robot = env.unwrapped.scene["robot"]
+            joint_pos = robot.data.joint_pos.cpu().numpy()  # Shape: [num_envs, num_joints]
+            joint_vel = robot.data.joint_vel.cpu().numpy()  # Shape: [num_envs, num_joints]
+            current_time = timestep * dt
+            
+            # Log data for each environment
+            for env_idx in range(env.unwrapped.scene.num_envs):
+                row_data = {
+                    'timestep': timestep,
+                    'time': current_time
+                }
+                
+                # Add joint positions
+                for joint_idx, joint_name in enumerate(robot.data.joint_names):
+                    row_data[f"{joint_name}_pos"] = joint_pos[env_idx, joint_idx]
+                    row_data[f"{joint_name}_vel"] = joint_vel[env_idx, joint_idx]
+                
+                joint_writers[env_idx].writerow(row_data)
+                # Flush to ensure data is written
+                joint_log_files[env_idx].flush()
+        
+        # log command data if requested
+        if args_cli.log_commands and timestep % args_cli.log_interval == 0:
+            robot = env.unwrapped.scene["robot"]
+            current_time = timestep * dt
+            
+            # Get velocity commands from the command manager
+            if hasattr(env.unwrapped, 'command_manager') and hasattr(env.unwrapped.command_manager, 'get_command'):
+                try:
+                    # Try to get velocity commands
+                    velocity_commands = env.unwrapped.command_manager.get_command("base_velocity")  # Shape: [num_envs, 3]
+                    velocity_commands_np = velocity_commands.cpu().numpy()
+                    
+                    # Get actual robot velocities
+                    actual_lin_vel = robot.data.root_lin_vel_b.cpu().numpy()  # Shape: [num_envs, 3]
+                    actual_ang_vel = robot.data.root_ang_vel_b.cpu().numpy()  # Shape: [num_envs, 3]
+                    
+                    # Log data for each environment
+                    for env_idx in range(env.unwrapped.scene.num_envs):
+                        row_data = {
+                            'timestep': timestep,
+                            'time': current_time,
+                            'cmd_lin_vel_x': velocity_commands_np[env_idx, 0],
+                            'cmd_lin_vel_y': velocity_commands_np[env_idx, 1], 
+                            'cmd_ang_vel_z': velocity_commands_np[env_idx, 2],
+                            'actual_lin_vel_x': actual_lin_vel[env_idx, 0],
+                            'actual_lin_vel_y': actual_lin_vel[env_idx, 1],
+                            'actual_ang_vel_z': actual_ang_vel[env_idx, 2]
+                        }
+                        
+                        command_writers[env_idx].writerow(row_data)
+                        # Flush to ensure data is written
+                        command_log_files[env_idx].flush()
+                        
+                except Exception as e:
+                    if timestep == 0:  # Only print once
+                        print(f"[WARNING] Could not access velocity commands: {e}")
+            else:
+                if timestep == 0:  # Only print once
+                    print("[WARNING] Command manager not accessible for logging commands")
+        
+        timestep += 1
+        
         if args_cli.video:
-            timestep += 1
             # Exit the play loop after recording one video
             if timestep == args_cli.video_length:
                 break
@@ -197,6 +320,18 @@ def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg | DirectMARLEnvCfg, agen
         if args_cli.real_time and sleep_time > 0:
             time.sleep(sleep_time)
 
+    # close joint log files if opened
+    if args_cli.log_joints:
+        for log_file in joint_log_files:
+            log_file.close()
+        print(f"[INFO] Joint logging completed. Files saved in: {joint_log_dir}")
+    
+    # close command log files if opened
+    if args_cli.log_commands:
+        for log_file in command_log_files:
+            log_file.close()
+        print(f"[INFO] Command logging completed. Files saved in: {command_log_dir}")
+
     # close the simulator
     env.close()
 
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py
index c9766e7d3a..2ff0cb384e 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py
@@ -54,3 +54,28 @@ gym.register(
         "skrl_cfg_entry_point": f"{agents.__name__}:skrl_rough_ppo_cfg.yaml",
     },
 )
+
+gym.register(
+    id="Isaac-Velocity-Stair-Unitree-Go2-v0",
+    entry_point="isaaclab.envs:ManagerBasedRLEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.stair_env_cfg:UnitreeGo2StairEnvCfg",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:UnitreeGo2RoughPPORunnerCfg",
+        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_rough_ppo_cfg.yaml",
+    },
+)
+
+gym.register(
+    id="Isaac-Velocity-Stair-Unitree-Go2-Play-v0",
+    entry_point="isaaclab.envs:ManagerBasedRLEnv",
+    disable_env_checker=True,
+    kwargs={
+        "env_cfg_entry_point": f"{__name__}.stair_env_cfg:UnitreeGo2StairEnvCfg_PLAY",
+        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:UnitreeGo2RoughPPORunnerCfg",
+        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_rough_ppo_cfg.yaml",
+    },
+)
+
+
+
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/flat_env_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/flat_env_cfg.py
index fbcb4b3e52..00d44eb8f2 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/flat_env_cfg.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/flat_env_cfg.py
@@ -4,6 +4,8 @@
 # SPDX-License-Identifier: BSD-3-Clause
 
 from isaaclab.utils import configclass
+from isaaclab.managers import RewardTermCfg as RewTerm
+from isaaclab.envs import mdp
 
 from .rough_env_cfg import UnitreeGo2RoughEnvCfg
 
@@ -14,9 +16,23 @@ class UnitreeGo2FlatEnvCfg(UnitreeGo2RoughEnvCfg):
         # post init of parent
         super().__post_init__()
 
-        # override rewards
-        self.rewards.flat_orientation_l2.weight = -2.5
-        self.rewards.feet_air_time.weight = 0.25
+        # ============ ANTI-CRAWLING REWARD MODIFICATIONS ============
+        # Significantly increase penalty for body tilting (was -2.5)
+        self.rewards.flat_orientation_l2.weight = -10.0
+        
+        # Add penalty for maintaining wrong height (prevents crawling low)
+        self.rewards.base_height_l2 = RewTerm(
+            func=mdp.base_height_l2,
+            weight=-5.0,
+            params={"target_height": 0.34}  # Go2's normal standing height
+        )
+        
+        # Increase penalties for unwanted movements to prevent crawling
+        self.rewards.lin_vel_z_l2.weight = -4.0    # Prevent bouncing (was -2.0)
+        self.rewards.ang_vel_xy_l2.weight = -0.2    # Prevent roll/pitch (was -0.05)
+        
+        # Enhanced foot rewards for proper walking (not shuffling/crawling)
+        self.rewards.feet_air_time.weight = 0.5  # Increased from 0.25
 
         # change terrain to flat
         self.scene.terrain.terrain_type = "plane"
diff --git a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/rough_env_cfg.py b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/rough_env_cfg.py
index 22fb69cff4..f0fab7f86a 100644
--- a/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/rough_env_cfg.py
+++ b/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/go2/rough_env_cfg.py
@@ -3,7 +3,10 @@
 #
 # SPDX-License-Identifier: BSD-3-Clause
 
+import torch
 from isaaclab.utils import configclass
+from isaaclab.managers import RewardTermCfg as RewTerm, SceneEntityCfg
+from isaaclab.envs import mdp
 
 from isaaclab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg
 
@@ -29,6 +32,13 @@ class UnitreeGo2RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
         # reduce action scale
         self.actions.joint_pos.scale = 0.25
 
+        # Body contact penalty - heavily penalize body touching ground
+        self.rewards.body_contact_penalty = RewTerm(
+            func=mdp.undesired_contacts,
+            weight=-10.0,
+            params={"threshold": 1.0, "sensor_cfg": SceneEntityCfg("contact_forces", body_names="base")}
+        )        
+
         # event
         self.events.push_robot = None
         self.events.add_base_mass.params["mass_distribution_params"] = (-1.0, 3.0)