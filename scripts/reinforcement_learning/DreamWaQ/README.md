# DreamWaQ Implementation in Isaac Lab

This repository contains an implementation of the **DreamWaQ** reinforcement learning pipeline for the Unitree Go2 robot using NVIDIA Isaac Lab. It leverages a custom RSL-RL runner to train a policy with a Context Encoder Network (CENet) for robust locomotion.

## Overview

DreamWaQ is a learning-based locomotion controller that uses a history of observations to estimate latent environment parameters and velocity commands. This implementation integrates the DreamWaQ architecture into the Isaac Lab framework, utilizing `rsl_rl` for the PPO algorithm.

## Directory Structure

```
DreamWaQ/
├── agent_cfg.py           # RSL-RL Agent configuration (PPO settings)
├── env_cfg.py             # Isaac Lab Environment configuration (Observations, Rewards)
├── train.py               # Training script
├── play.py                # Inference/Visualization script
└── rsl_rl_dreamwaq/       # Custom RSL-RL implementation
    ├── modules.py         # Neural Network Architecture (Actor, Critic, CENet)
    ├── on_policy_runner.py# Custom Runner Loop
    ├── ppo.py             # Custom PPO Algorithm
    └── storage.py         # Rollout Storage
```

## Environment Details

- **Task Name**: `Isaac-Velocity-Rough-Go2-DreamWaQ-v0`
- **Robot**: Unitree Go2
- **Terrain**: Rough terrain (variable height, stairs, slopes)

### Observation Space
The policy input consists of a concatenated vector of:
1.  **Proprioception**: Base angular velocity, projected gravity, joint positions, joint velocities, previous actions.
2.  **Latent/Context**: Generated by the Context Encoder Network (CENet) from observation history.

### Action Space
- **Joint Position Targets**: 12 DOF (3 per leg).

## Neural Network Architecture (`modules.py`)

The `ActorCritic_DWAQ` class implements the following components:
1.  **Context Encoder (CENet)**:
    -   Input: Observation history (`obs_history`).
    -   Output: Latent code (mean & variance) representing environment context and velocity estimation.
2.  **Actor**:
    -   Input: Current observation + Latent code.
    -   Output: Action distribution mean.
3.  **Critic**:
    -   Input: Critic observations (privileged information).
    -   Output: Value estimate.
4.  **Decoder** (Auxiliary Task):
    -   Reconstructs observations from the latent code to ensure meaningful representations.

## Usage

### Prerequisites
Ensure you have Isaac Lab installed and the environment configured.

### Training
To start training the policy:

```bash
# Run from the root of the Isaac Lab repository
./isaaclab.bat -p scripts/reinforcement_learning/DreamWaQ/train.py --task Isaac-Velocity-Rough-Go2-DreamWaQ-v0 --headless
```

**Arguments:**
- `--headless`: Run without the GUI (faster for training).
- `--video`: Record videos during training.
- `--max_iterations`: Number of training iterations (default: 100).

### Inference (Play)
To visualize the trained policy:

```bash
./isaaclab.bat -p scripts/reinforcement_learning/DreamWaQ/play.py --task Isaac-Velocity-Rough-Go2-DreamWaQ-v0
```

**Arguments:**
- `--num_envs`: Number of robots to spawn (default: 10).
- `--use_pretrained_checkpoint`: Load a pre-trained checkpoint from Nucleus (if available).

## Configuration

- **Environment**: Modify `env_cfg.py` to change observation scales, reward weights, or terrain difficulty.
- **Agent**: Modify `agent_cfg.py` to adjust PPO hyperparameters (learning rate, batch size, etc.).
- **Network**: Modify `rsl_rl_dreamwaq/modules.py` to change layer sizes or activation functions.

## Notes
- The implementation uses a custom `OnPolicyRunner` to handle the specific data flow required by the Context Encoder.
- Ensure `rsl-rl-lib` is installed and compatible (checked in `train.py`).
